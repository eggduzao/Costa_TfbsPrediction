In this chapter we provide the background information required for the understanding of this thesis. First, we introduce the necessary biological concepts (Section 2.1). Next, we present the biological experimental techniques which are the main sources of data used in our analysis (Section 2.2). Then, we introduce the problem which we are going to address in this thesis – the computational identification of active transcription factor binding sites (Section 2.3). Subsequently, we discuss the stateof-the-art computational solutions used to process modern biological experimental data and address the aforementioned problem (Section 2.4). Finally, we close this chapter with concluding remarks on the definitions made in this chapter and a brief motivation on our strategy to tackle the problem of identification of active transcription factor binding sites (Section 2.5).
In this thesis we will focus on the biological field of gene regulation. Such research area focuses on the understanding of the cellular mechanisms behind the temporal and spatial expression of different genes on different cellular conditions. We start this section by describing the basic concepts of molecular biology (Section 2.1.1). Then, we describe the main biological processes regarding gene regulation (Section 2.1.2). Finally, we discuss the role of chromatin dynamics on such regulatory processes (Section 2.1.3).  
In this thesis we focus on two important macromolecules which are found inside cells: proteins (composed by amino acids) and nucleic acids (composed of nucleotides). The proteins assume many roles: catalysis of chemical reactions (enzymes), metabolite processing, cell signaling, regulation of the production of more proteins, structural function and others. Given such great variety of processes in which proteins are associated with, one might regard these macromolecules as having a central role in the maintenance of living organisms. The nucleic acids can be of two types: the deoxyribonucleic acid (DNA) and ribonucleic acid (RNA). The main function of the DNA is to store the hereditary information of the organism. It is based on such information that new proteins are generated by celullar processes. The RNA have many important functions; however we will not focus on this molecule in this work. The DNA molecule is formed by a double helix of paired nucleotide chains, each of which composed of the nucleotide types: adenine (A), cytosine (C), guanine (G) and thymine (T). Each nucleotide is composed by a sugar (deoxyribose), a phosphate group and a nitrogenous base (which determines the nucleotide type). Within each DNA strand of the double helix, nucleotides are connected through phosphodiester bonds (strong covalent bonds). Between each DNA strand, nucleotides are paired and connected through hydrogen bonds (weaker than covalent bonds). Cytosines always pair with guanines and adenines always pair with thymines. Because nucleotides are paired between the double helix structure, is very common to refer to nucleotides as base pairs (bp). The Figure ?? depicts a graphical representation of the DNA molecule.  Proteins are chemical compounds with high molecular weight formed by a variable-length chain of amino acids. The amino acids that forms the proteins are composed of a central carbon atom which binds to a hydrogen, a carboxyl group, an amine group and a side chain. The side chain may be of various types and dictates the type of the aminoacid. There are 20 amino acid types which are more common to be found at proteins. The specific order of each amino acid type in a protein determines its three-dimensional structure. It is well-known that the protein’s function is directly related to its structure. The simple substitution of one amino acid in the proteic chain is sufficient to modify the protein three-dimensional conformation leading to a reduced functional capability or complete protein disfunction. The Figure ?? shows the different levels of protein structural conformation. The process in which proteins are created based on the information encoded in the cell’s DNA is called the “central dogma of molecular biology”. Here, the key parts of this process, which aid in the understanding of this work, are presented. These key parts are: (1) the initiation, (2) the transcription and (3) the translation. During the initiation phase, a number of proteins called transcription factors bind in the DNA and recruit another protein called RNA polymerase (Figure ??a). The DNA region in which these transcription factors and RNA polymerase bind to start transcription is called promoter. Then, in the transcription phase, the RNA polymerase scans the DNA and creates an RNA molecule, based on the information encoded in the DNA (Figure ??b). The part of the DNA which is transcribed by the RNA polymerase is called gene. Finally, in the translation phase, the newly-generated RNA migrates outside the cell’s nucleus and a protein called ribosome scans the RNA and creates a protein molecule based on the information encoded in the RNA (Figure ??c). The rate in which the transcription occurs for a particular gene is called the gene’s expression. It is important to mention that, although only one of the DNA strands is read during the transcription process, both strands contain information necessary to produce RNA. Another important issue is the orientation of these two DNA strands. Each strand has two extremities: one corresponding to a hydroxyl group attached to the 30 carbon atom of the sugar; and the other corresponding to the phosphate group attached to the 50 carbon atom of the sugar. For this reason, processes involving the sliding of proteins in DNA have two orientations: forward (50 → 30 ) and reverse (30 → 50 ). The different strands in the DNA helix are attached to each other in opposite (anti-parallel) orientations. The transcription always occurs in the forward orientation.  
Living organisms can be divided into two groups: (1) prokaryotes – the organism’s genetic material (i.e. the DNA) is located in the cell’s cytoplasm and presents a circular structure and (2) eukaryotes – the organism’s cells have a nucleus which contains, among other structures, the DNA. Here, we will focus on eukaryotes. Most eukaryotes have more than one DNA molecule, which are termed chromosomes. The set of a cell’s chromosomes is called genome. Furthermore, some organisms present more than one copy of a chromosome. The human genome – which is the main focus of this work – presents two copies of 22 different chromosomes (refered to based on a number between 1– 22) and two additional chromosomes (termed X and Y), which determines the individual’s biological gender. This adds up to a total of 46 chromosomes and classify humans as diploid organisms (have two copies of each chromosome). The transcription initiation was previously described as the step in which the RNA polymerase binds to the promoter region in order to start the process of transcription. Nevertheless, there are many factors that contribute to the expression of particular genes in particular types/stages/conditions of a cell. We call “gene regulation” the wide range of mechanisms that are used by cells to increase or decrease the production of specific gene products. Gene regulation may happen in different stages of the central dogma. However, most part of the regulatory events happens at the transcription initiation level. A major role of the regulation at this level is played by proteins termed transcription factors,  which use their physicochemical properties to direct the intensity level in which gene products are created. The transcription factors bind to DNA regions called transcription factor binding sites which can be close (promoter region; <∼1, 000 bp from the transcription start site) or far (distal regulatory regions; up to ∼1, 000, 000 bp) from the gene. Different transcription factors may bind to different transcription factor binding sites to increase or decrease the expression of genes. The Figure ?? shows a graphical representation of a basic regulatory landscape of a gene. The number of regulatory elements vary between genes; however the construct of transcription factor and their DNA binding sites are generally present. The transcription factors contain a specific part (formally called “domains”) within their structure, termed active site, which enables them to bind to the DNA. There is a relatively short number of smaller structural variants (which compose the final protein structure) in comparison to the number of different protein types. Some of these structural variants, including the ones containing active sites, are repeated between different protein. These DNA-binding protein domains usually have affinities towards specific DNA sequences. These affinity sequences are termed “DNA motifs”. The Figure ?? shows four DNA-binding protein domains and examples of proteins that contain such domains and their respective DNA binding affinity motifs.  
The DNA is not isolated in the cell nucleus. Instead, it is found wrapped in proteic complexes which is associated to the compaction of the DNA within the cell nucleus and many regulatory events. Such DNA+protein structure is termed chromatin. The DNA is found wrapped in a set of eight proteins called histone complex, which are generally composed of four pairs of histones named H2A, H2B, H3 and H4. The unit composed by the DNA wrapped in approximately 1.65 turns (∼147 bp) around the histone complex is called nucleosome. From this lower level structure (nucleosome) the chromatin structure can be compacted in many different levels. This compaction organization is depicted in Figure ??. Briefly, the chromatin can be found in a very condensed structure which does not allow transcription initiation (termed heterochromatin, or simply “closed chromatin”); or in a decondensed form, allowing transcription initiation and gene expression (termed euchromatin, or simply “open chromatin”). Different parts of the genome can be open and closed at different times, allowing a specific set of genes to be expressed under different cell conditions. This is one of the main mechanisms in which we are able to observe such a high number of different cells, each of which expressing a different set of genes, given that they all share the same underlying genomic information encoded in the DNA. The chromatin can switch between closed and open states via two major chromatin remodelling mechanisms: (1) covalent post-translational histone modifications by specific enzymes such as histone acetyltransferases (HATs), deacetylases, methyltransferases and kinases and (2) ATPdependent chromatin remodelling complexes which either move, eject or restructure nucleosomes. Here, focus on the post-translational histone modifications. The histone proteins’ n-terminal usually protrudes from the nucleosome and is termed histone tail. These histone tails can undergo post-translational chemical modifications at specific amino acids. These modifications include the methylation (addition of a methyl group), acetylation (addition of an acetyl group), phosphorylation (addition of a phosphoryl group), ubiquitylation (addition of a ubiquitin protein) and sumoylation (addition of SUMOs – small ubiquitin-like modifiers). These modifications have a specific nomenclature dictated by: histone type, amino acid type, amino acid position within the histone tail and modification type. For instance, “H3K4me1” refers to the monomethylation (me1) of the fourth lysine (K4) of the tail of histone H3. The histone modifications are directly associated to regulatory events since they change the accessibility of proteins to the DNA in the chromatin, enabling or disabling the binding of TFs. For instance, the HATs are able to transfer an acetyl group to certain amino groups of histone tail’s ly-  
between histones and DNA weaker. Consequently, the DNA is more accessible for TF binding. The Figure ?? displays the known effects of modifications in lysines in the tail of histone H3.  
Recently, novel DNA sequencing platforms have enabled the sequencing of a very large number of DNA fragments (up to a few billions) on one single assay with a significant decrease in cost and complexity.1 However, although these techniques are able to sequence a very large number of DNA fragments per single execution; these fragments must be small (usually up to hundreds of bp). We refer to these novel sequencing platforms as next-generation sequencing (NGS).4 Since the development of the first NGS technologies,5 they have been constantly improving. A complete discussion on NGS can be found at Rusk et al.6 The emergence of NGS and its constant technological improvements have enabled the revisiting of traditional biological assays to investigate regulatory elements (described in Section 2.1.2) using the cell-specific chromatin dynamics context (described in Section 2.1.3). On revisiting such methods, their protocols could be adapted in order to fit the NGS technologies, which enables them to be performed in a genome-wide manner. Such large-scale analysis has potential to reveal the highdimensional relationships between regulatory elements. NGS-based assays have enabled multiple current research progress, which unraveled the regulatory mechanisms linked to conditions, such as cell differentiation or the onset of diseases, of multiple cells. In this section we will describe the following techniques: (1) Chromatin immunoprecipitation followed by NGS (ChIP-seq; Section 2.2.1) and (2) DNase I footprinting followed by NGS (DNase-seq; Section 2.2.2).  
The ChIP-seq technique7 consists on fetching target DNA-bound proteins and further sequencing of the DNA fragments fetched using NGS techniques. These target proteins can be, for instance, transcription factors or histones with a particular post-translational modification. This allows the genome-wide identification of the genomic regions in which a target protein is bound within a single experimental execution. When applied to a target transcription factor, the ChIP-seq experiment allows us to identify the transcription factor binding sites. When applied to histones with particular post-translational modification, the ChIP-seq experiment allows us to identify the genomic regions in which these modified histones occurr, and therefore make inferences on that region’s particular chromatin structure. The ChIP-seq protocol starts by isolating the nuclei of cells and breaking them in order to access the genomic material (chromatin). The isolated genomic material is cross-linked in order to preserve all protein-DNA binding events. Next, the crosslinked chromatin is sheared into approximately 200 bp DNA fragments with any massive DNA shearing procedure. Afterwards, the chromatin lysate is treated with an antibody that targets a particular protein of interest. The solution is then immunoprecipitated. In this procedure, we fetch only the sheared chromatin fragments that contains the protein of interest. The immunoprecipitated solution is separated and washed in order to keep only the DNA fragments. Then, these DNA fragments are sequenced using a NGS technique. It is important to mention that only the beginning of the fetched DNA fragments are sequenced (50–100 bp) by NGS techniques. Such process is depicted in Figure ??a–c. The sequenced DNA fragments (termed “reads”) can be mapped back into the reference genome using massive string alignment algorithms (Figure ??d), which are developed specially for mapping short DNA reads (length of 50–100 bp) into a big reference genome (human genome length is ∼3.1  
many available algorithms such as Bowtie 28 or the Burrows-Wheeler Aligner (BWA).9 Given these aligned reads, we are able to generate a genomic signal by calculating the overlap between these reads at every genomic coordinate, i.e. every base pair of the genome (Figure ??e). Nevertheless, since only the first 50–100 bp of the fetched fragments are sequenced, they need to be extended to the approximate total size of the fetched fragments (approximately 200 bp). This extension step reflects the fact that the protein can be bound to virtually any location of the fetched DNA fragment. Finally, we are able to identify the binding locations of the target protein by identifying regions significantly enriched with the genomic signal (Figure ??f). This is also a computationally demanding problem which was solved with the development of modern genomic peak-calling algorithms such as the model-based analysis for ChIP-seq (MACS).10 Since the ChIP-seq signal has a low resolution, i.e. it is smoothed given the fact that we have to extend the aligned reads, the target protein is considered to be likely bound anywhere whithin the called peaks.  
The DNase-seq technique11, 12 consists in the observation of the DNA digestion by a certain cleavage agent able to break the DNA molecule. The cleavage agent used in this method is the enzyme deoxyribonuclease I (DNase I). The rationale of this method is that the DNase I enzyme can cleave the DNA in regions where it is accessible (i.e. open chromatin). Furthermore, within open chromatin regions, the DNase I enzyme will only be able to cleave the DNA at protein-free regions, leaving “footprint marks” that can be traced back as DNA-bound regulatory proteins. The DNase-seq protocol starts by isolating the nuclei of cells and breaking them in order to access the genomic material (chromatin). The isolated genomic material will be treated with optimal concentrations of DNase I, which will cleave the chromatin at random accessible positions. These accessible positions are the chromatin regions in which the DNA is open (i.e. not fully wrapped around the histone complexes) and protein-free (i.e. not being bound by proteins such as transcription factors). Such cleaved DNA fragments can be isolated and sequenced using the same algorithms as described for the ChIP-seq procedure (Section 2.2.1). Such process is depicted in Figure ??a–e. Then, we are able to create a genomic signal by counting the number of overlapping reads at every genomic position. In the DNase-seq case, we only count the 50 position of the reads, since that is the position in which the DNase I enzyme has cleaved the DNA and indicates an open chromatin region (Figure ??f). The resulting genomic signal will represent a base-pair resolution map of the open chromatin positions within the whole genome. Finally, we are able to detect the DNase-seq enriched regions by using algorithms specially designed for such purpose such as the F-seq13 (Figure ??g). Each region enriched with DNase-seq reads – termed DNase I hypersensitivity sites (DHSs) – are composed of several DNase-seq signal peaks. Note that the depletions within two of these base-pair resolution peaks are indicative of a region which the DNase I enzyme could not access because there was probably a protein binding in that region. These depleteions between two peaks are called “footprints” and the identification of these regions gives us a genome-wide map of putative active transcription factor binding sites. It is important to point the differences between DNase-seq and ChIP-seq. In the DNase-seq method, we determine the binding of any protein in the region being analyzed, without knowing which protein is binding; however in ChIP-seq we only determine the binding of a particular target protein with a known antybody in the region of interest. Furthermore, while the DNase-seq can provide the precise protein binding location, the ChIP-seq tells us an approximated region for the binding of the target protein, since the protein can be virtually anywhere within the ∼200 bp immunoprecipitated fragments. The selection of the technique to use depends mainly on the experimental design and should consider these important details.  
In this section we define the problem we are going to address in this thesis - the computational identification of transcription factor binding sites. First, we define the problem (Section 2.3.1). Then, we make a brief discussion on why this is an important subject of study (Section 2.3.2). In sequence, we discuss a computational sequence-based method for the identification of transcription factor binding sites and its inability to differentiate between active and inactive binding sites (Section 2.3.3). Such discussion provides some insight on the difficulty of active transcription factor binding site identification. Finally, we show how we can use the biological experimental assays DNase-seq and ChIP-seq to provide information necessary to differentiate active from inactive transcription factor binding sites (Section 2.3.4).  
In Section 2.1.2, we presented an overview of main features regarding eukaryotic gene regulation. One of the main features presented are the transcription factors, which are proteins that bind to specific genomic regions called transcription factor binding sites. Furtermore, the transcription factors binding process is highly dynamic and varies between different cells and cellular conditions given the chromatin dynamics context (Section 2.1.3). Such dynamic regulatory process colaborates with the orchestration of proper spatial and temporal expression of genes. Therefore, the identification of such regulatory elements is crucial to understand regulatory networks driving multiple cellular processes and the onset of diseases which manifest due to regulatory network disruption. Given that, we can formally define the problem we are addressing in this work: Active Binding Site Detection Problem: For a given cell’s genome, find all putative genomic regions (termed transcription factor binding sites; TFBSs) in which there are proteins (termed transcription factors; TFs) binding, playing a specific regulatory role in the gene expression of the cell. The problem of active binding site detection has a few variants, depending mainly on the specific goals of research analyses. One the one hand, system-focused analyses require the identification of a few target binding sites. In this case, there is a priori knowledge on the particular regulatory players involved at the particular mechanism being studied. Such problem is usually addressed by biological experimental assays such as ChIP-seq (Section 2.2.1) for the target transcription factors if interest. On the other hand, system-exploratory analyses require a genome-wide map of all putative transcription factor binding sites that are being bound by proteins at a particular cell condition. In this case the researchers are interested in a genome-wide map of all putative active binding sites in order to perform various further inferences on the regulatory dynamics of the system under study. In this case, the problem can be addressed by biological techniques such as DNase-seq (Section 2.2.2).  
As previously mentioned, the identification of regulatory elements is a very important task, since they are the key players on most regulatory mechanisms. The subject under study can be a single cell’s regulatory landscape or the mechanisms behind a system of two or more different cells. In the single cell’s study we might be interested, among other, in the identification of regulatory elements associated with: (1) up/downrelgulation of a gene or group of genes; (2) group of genes interesting for a particular reason such as drug response; (3) viral infection and (4) cell signalling mechanisms. When studying more than one cells, on might be interested, for instance, in the detection of the regulatory elements related to: (1) cellular processes such as differentiation, apoptosis, aging and  development; (2) cellular reponse to stimulus by studying the regulatory elements disposition before and after stimulus; (3) temporal cellular response to personalized therapies and (4) the mechanisms underlying diseases by comparing disease-affected cells with healthy cells. There are a great number of studies that benefited from the proper identification of active binding sites. Recent studies, that uses NGS-based experimental techniques coupled with proper computational frameworks, have particularly provided multiple insights on a different range of mechanisms such as cell differentiation and understanding the onset of diseases. For instance, studies were able to: (1) unravel cellular differentiation processes;14, 15 (2) unravel disease mechanisms;16–18 (3) perform gene expression prediction and high-order functional analyses19–21 and (4) understand other cellular regulatory elements such as long noncoding RNAs.22, 23 In summary, the identification of active transcription factor binding sites is important because of its broad impact on many other cellular processes. However, there are many issues that makes this a hard problem. First, the regulatory landscape is cell condition-specific, i.e. each cell undergoing a specific stage/process/condition/stimulus contains a different set of regions accessible by TFs. Also, there are over 1, 500 known TFs, each of which can bind to the DNA directly or by being recruited (such as co-activators). The possible combinatorial binding framework makes it very hard to unravel regulatory regions on a TF-wise approach. Furthermore, some TFs bind in a very dynamic manner, which makes hard to interpret a particular snapshot of the regulatory landscape captured at a specific moment of the cell’s lifetime.  
With the increasing demand for methods that are able to analyze the whole genome and given the fact that biological experimental assays are highly technical and time-consuming, some computational DNA sequence-based approaches were developed in order to aid the genome-wide search of transcription factor binding sites. One of the first sequence-based computational approaches consisted on a string-matching algorithm that used information regarding the transcription factor’s DNA sequence binding affinity (i.e. transcription factor DNA motif) to build mathematical models that could be used to search for similar sequences in the genome. Such algorithms are called “motif matching”. The transcription factor binding sites predicted using sequence-based computational methods are called motif-predicted binding sites (MPBSs). Some versions of computational sequence-based method have very reasonable accuracy rates and their low computational complexity makes it much easier to apply than the highly-technical and timeconsuming biological assays. However, this technique has many intrinsic disadvantages. First, protein affinity motifs are generally small (usually between 5–20 bp) and degenerate (only a fraction of the motif is highly conserved). Second, protein affinity motifs are not available for all TFs. One of the main reasons for that is the bottleneck generated by the requirement of a priori biological assays to determine such proteins’ affinities. Third, the quality of such a priori biological assays has a direct impact on the performance of computational sequence-based methods. Fourth, some transcription factors bind in the genome through other co-factors. Therefore, the creation of sequence affinity models for these transcription factors is very hard since these co-binding partners might change under different circumstances. Fifth, some transcription factors do not bind directly on the DNA, they only have domains that interact with other proteins binding on the DNA. Thus, it is impossible to create a sequence-based affinity model for such transcription factors. Sixth, the accuracy of sequence-based methods highly depends on the many parameters used to generate the affinity model and to apply such model in the genome. However, the main disadvantage of computational sequence-based methods is the fact that they are unable to identify active binding sites, i.e. binding sites that are actually being bound by proteins at a specific cellular condition. This happens because computational sequence-based methods rely  
different cells for a particular organism; independent of cell type, cellular condition, life stage, stimuli response, and others. What changes between the cells of an organism is the chromatin structure. As previously mentioned, different parts of the genome can be open and closed at different times, allowing a specific set of genes to be expressed under different cell conditions. This is one of the main mechanisms in which we are able to observe such a high number of different cells, each of which expressing a different set of genes, given that they all share the same underlying genomic information encoded in the DNA. The Figure ?? shows a graphical example of two cells at different stages of commitment. Although the genomic region (locus) depicted is the same for these two cells, one present a closed chromatin structure, while the other present an open chromatin structure. The closed chromatin observed for the long-term hepatopoietic stem cell (Figure ??a) does not allow the gene ATF3 to be transcribed, while the open chromatin structure present in the monocyte cell (Figure ??b) does allow the expression of ATF3 gene, since the transcription factors and transcription machinery are able to access that region and start the transcription process. Therefore, computational sequence-based methods are not able to capture the active (cell-specific) binding sites. In practise, this is usually expressed as a very high number of false positive predictions, which represent the transcription factor binding sites which are not being accessed in a particular cellular condition.  
In order to make predictions of active transcription factor binding sites, we must use chromatin dynamics information that provides the required cell-specificity. Such information can be obtained with the biological assays discussed in Section 2.2. There is a well-known pattern that can be seen in genomic signals from DNase-seq and histone modification ChIP-seq. We will call this pattern the “grammar of transcription factor binding sites”. This grammar, depicted in Figure ??, shows that transcription factor binding sites happen at depletions between two peaks of the DNase-seq signal. These peaks of DNase-seq signal, which comprise a DHS, occur whithin a depletion between two peaks of activating histone marks. Such pattern can be used in order to make better predictions of active binding sites, when compared to purely sequence-based methods (Section 2.3.3). However, the very high magnitude and complexity of the genomic signals generated with DNase-seq and ChIP-seq require special computational frameworks. Such computational frameworks which processes NGS-based data gained popularity over the last years and can be used to address the problem of active transcription factor binding site prediction.  
In this section we show how the state-of-the-art computational methods use the grammar of active transcription factor binding sites to provide predictions of active transcription factor binding sites. These computational methods are termed computational footprinting methods (Section 2.4.1). Then, we will define the different types of computational footprinting methods (Section 2.4.2). Next, we show the main challenges on the identification of active transcription factor binding sites using these modern computational approaches (Section 2.4.3). Finally, we close this section with a comprehensive literature review on published computational footprinting methods (Section 2.4.4).  
In this thesis we will focus on computational footprinting methods to address the problem of active transcription factor binding site prediction. We define such approach as follows:  
We should clarify some parts of the above definition. First, the “nucleotide-resolution” part refers to the fact that the predicted TFBS regions will be as close as possible, in terms of genomic position and predicted region’s width, to the real TFBSs. In other words, the method should have a high spatial specificity. Second, the “genome-wide” part means that the computational framework is capable of executing within a reasonable amount of time with data from all genomic coordinates. Finally, the term “map of active TFBSs” refers to the output of these computational frameworks. This output should consist of multiple genomic regions, each of which starting and ending at particular genomic coordinates, which represents the putative active binding sites. At this point we make an important clarification. The ChIP-seq method can be obviously applied to provide a very reliable genome-wide map of the binding of a particular target TF. Then, one common question is: why using complex computational approaches to process DNase-seq and histone modification ChIP-seq if one can directly assess a transcription factor binding site map using ChIPseq for TFs? There are several reasons for using computational footprinting methods as a research tool. First, the ChIP-seq relies on the quality of the antibody used on the immunoprecipitation step. There are many TFs in which the antibodies do not work properly or do not work at all. Second, if the experimental design relies on the identification of a very small number of TFs (system-focused experiment design) then ChIP-seq for TFs might be a good choice; however, if one is interested in a higher number of TFs (system-exploratory experiment design), the number of TF ChIP-seq assays makes the study very expensive and time consuming. Third, computational footprinting methods provide a map of active TFBSs without restrictions for known transcription factors, creating the possibility to identify novel transcription factors or novel binding mechanisms. In summary, the idea of using computational footprinting methods is to use the lowest number of assays possible in order to generate a robust active TFBS map. Therefore, the idea is to use experimental data that delineates open chromatin regions such as DNase-seq and ChIP-seq for histone modifications (instead of specific target TFs).  
Computational footprinting methods can be broadly categorized as: (1) segmentation methods and (2) site-centric methods. The main difference between these two groups of computational footprinting methods is the usage of a priori predictions of TFBSs based on DNA sequence affinity, i.e. motifpredicted binding sites (MPBSs; Section 2.3.3). The rationale behind segmentation methods is to use NGS-based data to scan the genome for footprints. This approach generates a map of putative active binding sites without actually specifying the transcription factors that are binding these regions. The disadvantage is that further processing is necessary in order to identify such transcription factors. However, the advantage of such approach is that novel transcription factors or binding types can be detected. The rationale behind site-centric methods is that the analysis starts with putative binding sites obtained by using, for instance, a sequence-based prediction method such as motif matching (Section 2.3.3). Then, experimental evidence around these a priori predictions are gathered and classified using machine learning methods in a supervised or unsupervised fashion. This approach leads to footprints for target TFs. The advantage of such approach is that we already know which TFs are binding to the correctly classified footprints. However, the disadvantage is that it depends on this a priori TF evidence, which is not always available. Furthermore, de novo motif finding is virtually impossible on footprint predictions obtained with site-centric methods, while it is easily performed on the footprint predictions of segmentation approaches. The Figure ?? provides a graphical representation of the segmentation and site-centric approaches.  
It is known that NGS-based genomic signals are very noisy and intrinsically complex. Most methods used some sort of smoothing approach to handle such complexity.24–27 Although a few attempts have been made to use these signals in their maximum possible resolution,28, 29 not much attention was given to the proper mathematical modelling of these complex signals. The NGS-based genomic signals can be affected by multiple artifacts stemming from either the biological protocol or the computational pre-processing steps. These artifacts were summarized recently by Meyer et al.30 Furthermore, none of the computational footprinting methods so far took full advantage of the nucleotide-resolution signal generated for DNase-seq and histone modifications ChIP-seq signal. The current integrative approaches usually takes only the nucleotide-resolution signal for one of the signals, such as DNase-seq in the Centipede24 method. The main reason discussed by most integrative approaches24, 25 is that it is hard to integrate signals which has a high degree of variation. Moreover, there is no well-defined gold standard for the evaluation of footprinting methods. All work so far has used ChIP-seq of TFs in conjunction with motif-based predictions (MPBSs) as ground truth. In short, MPBSs supported by ChIP-seq peaks are positive examples (true TFBSs), while MPBSs without ChIP-seq support are negative examples (false TFBSs).25 This evaluation requires TF ChIP-seq experiments to be carried out on the very same cells as the DNase-seq experiment and has a few caveats. First, TF ChIP-seq peaks are also observed in indirect binding events.31 Second, they have a lower spatial resolution than DNase-seq. Therefore, false TFBSs might be regarded as true TFBSs by proximity to a real TFBS of a distinct TF.25, 31 Also, He et al.32 showed that the DNase-seq sequence cleavage bias around TFBSs strongly affects the performance of a computational footprinting method (FS) in a TF-specific manner. Such impact is depicted in Figure ??. They also indicated several TFs, such as nuclear receptors and de novo motifs found via computational footprinting,33 where the DNase-seq profile resembles their sequence cleavage bias estimate. Furthermore, they indicated that ranking putative TFBS by the number of DNase-seq reads around putative TFBSs (TC) outperforms the ranking by FS, which is used by some computational footprinting methods.33, 34 Besides the sequence cleavage bias, another experimental aspect affecting the computational analysis of DNase-seq is the residence time of TF binding. Sung et al.29 showed that short-lived TFs display a lower DNase I cleavage protection pattern, i.e. low number of DNase-seq reads surrounding the footprint (see Figure ??). Such fleeting transcription factors would not be detectable by methods that search for such protection pattern on DNase-seq data.  
A number of computational footprinting methods have been proposed. These methods used different NGS-based experimental data sources, different algorithms and targets different experiment designs. Here, we will discuss the main published methods, providing a comprehensive literature review on computational footprinting methods. A summary of the main computational footprinting methods and their features is presented in Table 2.1. Table 2.1: Overview of methods. We list here the main characteristics of the evaluated methods. Methods are characterized by their type (SC – site centric versus SEG – segmentation approach), algorithm, bias correction strategy, resolution/smoothing strategy, method for ranking footprints, availability and usability. Concerning availability, methods obtain a ‘+’ if they are public available (‘–’ otherwise). Boyle method is not public, but authors provide footprint predictions of a few cells. The code for Neph was obtained upon request to authors. Concerning usability, methods natively supporting standard genomic files and being executed with few commands (≤ 3) have ‘+’ (‘–’ otherwise).  Name  Type  Algorithm  Bias Correction  Resolution Smoothing  Footprint Ranking  Availa- Usability bility  Others  BinDNase  SC  Logistic Regression  No  bp / Sliding Window  Probability  +  –  Boyle Centipede  SEG SC  HMM Bayesian Mix. Model  No No  bp bp  None Probability  – +  – –  Cuellar  SC  no  +  –  SEG  4-mer  Sliding Window bp  PWM Score  DNase2TF  p-values  +  +  FLR  SC  6-mer  bp  Log-Odds  +  –  HINT-BC  SEG  Weighted Motif Match Sliding Window Mixture Model HMM  6-mer  bp  TC  +  +  Neph  SEG  bs  FS  –  –  PIQ  SEG  Sliding Win- no dow GP/Expectation No Propagation  bp / GP  Probability  +  +  Wellington  SEG  Sliding Window  bp  p-value  +  +  No  Require ChIP-seq Training  TF for  Integrates Histone and Sequence Data  Bias Correction for Each TF Integrates Histones  Support Replicates, Time Series  
One of the first attempts to create a computational footprinting method for DNase-seq data was performed by Hesselberth et al.34 In their study, they performed the DNase-seq experiment in the Saccharomyces cerevisiae organism (yeast). In posession of the DNase-seq signal, they used a three-phase segmentation approach to detect footprints in the DNase-seq data. In the first phase, they considered every possible window k = [kmin , kmax ] that was contained within one of the specified target regions (DHSs) and computed a depletion score for each of these regions. In the second phase, they selected high-scoring windows using a greedy algorithm, eliminating from consideration any window that overlapped a window with a higher score. Finally, in a third phase, they shuffled the input data independently within each target region and repeated the entire procedure, using the resulting scores to estimate q-values. They introduced the “footprint score” (FS) as a depletion score. The FS is defined as the ratio between the amount of DNase I cleavage within the footprint and the amount of DNase I cleavage in the footprint’s flanking regions. More formally, let the interval k = [m, n] be a putative footprint region and x = hx1 , ..., xG i be the DNase-seq genomic signal from a genome of size G. The FS can be calculated as   nC + 1 nC + 1 FSk = − + , (2.1) nR + 1 nL + 1 where 2n−m  n  nC =  ∑  j=m  x j,  nR =  ∑  j=n  m  x j,  nL =  ∑  
 
 j=2m−n  
features regarding such computational footprinting. They identified many known sequence motifs in these footprints, observing that collectively, 35.2% of the footprints with a false discovery rate of 0.05 overlapped a conserved factor binding site inferred from ChIP data. Furthermore, they observed that the patterns of DNase I protection surrounding different TFs had different average shapes, i.e. the DNase-seq average signal varies depending on the binding type of TFs. Finally, they created a very consistent genome-wide map of transcription factor binding sites for the Saccharomyces cerevisiae,  
Whitington et al. & Won et al. Two other computational footprinting methods by Whitington et al.35 and Won et al.36 used ChIPseq for histone modifications in order to assess TFBS footprints. In Whitington et al.35 they used data regarding histone modification H3K4me3 to infer TFBSs for 14 mouse TFs and 10 human TFs. They used a site-centric approach that consists on simple filters for MPBSs. These filters consist on regarding a MPBS as true if the density of H3K4me3 was above certain ad hoc thresholds. They showed that using histone modification H3K4me3 information to filter out false positive MPBSs significantly improves the accuracy in comparison to the unfiltered MPBSs alone. Furthermore, they showed that H3K4me3 filters outperformed other filters based on proximity to transcription start sites (TSSs) or phylogenetic conservation. Won et al.36 used a more complex approach – termed Chromia – which detects footprints with HMMs in a site-centric manner. Chromia uses a multivariate HMM which integrates continuous H3K4me3 signals and PWM scores. Their HMM models have three components, which are trained separately: (1) a promoter-proximal component – trained around TSS regions with strong h3K4me3; (2) a distal enhancer component – trained around p300 binding sites with strong H3K4me2 and H3K4me3 signals and (3) background – trained in the whole chromosome 1. The authors showed that Chromia significantly outperformed competing approaches in a gold standard TFBS set created using 13 TFs binding evidence (ChIP-seq and RNA interference knockdown) binding in mouse embryoinic stem cells. Neph et al. Neph et al.33 used a simplified version of the segmentation-based method originally proposed in.34 Their method consists on applying a sliding window to find genomic regions (6–40 bp) with low DNase-seq signal between regions (3–10 bp) with high DNase-seq signal (peak-dip-peak pattern). They performed their experiments on human DNase-seq data generated using the double-hit protocol. They also evaluate the footprint score (FS) to determine the most significant predictions. Their study amplified the analysis scale significantly, by detecting footprints for 41 diverse human cells with data from the ENCODE repository.3 Such a large-scale study was able to provide multiple new insights on computational footprinting. First, they found that genetic variants affecting allelic chromatin states are concentrated in footprints, and that these elements are preferentially sheltered from DNA methylation. Second, they showed that the average TF-wise patterns of DNase I digestion can be correlated to the crystallographic topography of protein-DNA interfaces, indicating that TF structure has been evolutionarily imprinted on the genome. Finally, they performed an extensive “brute force” de novo motif finding algorithm and found 683 unique motif structures, of which 394 (58%) matched distinct experimentally-verified motif models present in Jaspar,37 Uniprobe38 and Transfac39 motif repositories. Boyle et al. Boyle et al.28 designed a segmentation computational footprinting approach, which is based on using hidden Markov models (HMMs) to predict the DNase-seq pattern described in Figure ??. They performed their experiments on human DNase-seq data generated using the single-hit protocol. Briefly, their HMM uses a normalized DNase-seq cleavage signal to find regions with depleted DNase I digestion (footprints) between two peaks of intense DNase I cleavage. As the DNase-seq profiles required a nucleotide-resolution signal, which is usually noisy, the authors used a Savitzky-Golay smoothing filter to reduce noise and to estimate the slope of the DNase-seq signal.40 Their HMM had five states, with specific states to identify the decrease/increase of DHS signals around the peak-dip-peak region.  
different cells. Second, they described a conservation phenomenon which was not observed in the conservation study performed by Hesselberth et al.34 They find that for most TFs, there is a marked drop in conservation ∼10 bp immediately flanking the footprint. Beyond this drop, conservation increases again before gradually decreasing to background levels, creating a “shoulder” in this signal. They also described in details the unique binding characteristics that the human insulator CTCF footprints displays. Finally, they used the STAMP41 method to detect putative TFs in footprints, which simply searches for TFs on known DNA sequence affinity information. Therefore, a de novo motif finding approach was not performed as in Neph et al.33 Pique-Regi et al. (Centipede) Nevertheless, one of the most used footprinting approaches was created by Pique-Regi et al.24 Their stratey, termed Centipede, is a site-centric approach, which gathers experimental and genomic information around motif-predicted binding sites (MPBSs). It then uses a Bayesian mixture model as an unsupervised classification tool to label each retrieved MPBS as either ‘bound’ or ‘unbound’. Their approach was the first to integrate multiple different experimental assays. The experimental includes DNase-seq and histone modification ChIP-seq. The DNase-seq data was used at the nucleotideresolution, by fetching raw DNase-seq signal surrounding a 200 bp window around each MPBS. However, only the average histone modification ChIP-seq signal was used. The genomic data used includes PWM bit-score, sequence conservation and distance to the nearest TSS. They evaluated their approach on six TFs with ChIP-seq data available. Their gold standard was created as follows. First, they overlapped the MPBSs with TF ChIP-seq peaks. If a MPBS overlapped with a ChIP-seq peak, it was considered a “true MPBS”; otherwise, it was considered a “false MPBS”. Then, they were able to quantify the amount of true positive, false positive, true negative and false negative predictions by overlapping their footprints with the true/false MPBS gold standard (see Section 4.6.3). With such gold standard they were able to calculate receiver operating characteristic (ROC) curves and evalute the area under the ROC curve (AUC). Their reported average AUC for the six tested TFs were as high as 98.11%. However, they did not observe a gain in accuracy when using a model with both DNaseseq and histone modification ChIP-seq (median AUC = 96.52%). Furthermore, although their method is site-centric they were able to devise a de novo motif finding algorithm. However this algorithm is time consuming and relies on the usage of a priori information of genomic k-mer distributions. Cuellar-Partida et al. Cuellar-Partida et al.25 proposed a site-centric method to include NGS-based experimental data as priors for the motif matching procedure (see Section ??). Their method uses a probabilistic classification approach inspired in Bayes decision theory to compute better log-posterior odds scores than the ones observed by purely using the PWM structure. Before the computation of prior probabilities the DNase-seq or histone modification ChIP-seq signals are smoothed as follows. To create smoothed DNase-seq signals they evaluated the number of reads aligning to a window of 150 bp, specified every 20 bp. Histone modification smoothed signal input data was specified in a 25 bp resolution. In every position, it was summed 1 if a mapped read fell within 0–200 bp from the 25 bp window and 0.25 if it occurred within 200–300 bp. After the computation of prior probabilities with smoothed DNase-seq signal, they perform the motif match using the program FIMO.42 They performed the first comparative study on computational footprinting methods, where they compared their method with Centipede and with a much simpler approach termed here as “tag count” (TC). This site-centric approach corresponds to the number of DNase I cleavage hits within a window of length L around a certain genomic interval (in this case, MPBSs). More formally, let the interval k = [m, n] be a putative  
TC can be calculated as m+n L 2 + 2 −1  TCk =  ∑  
 
 L j= m+n 2 −2  Their results showed that, for their approach, the DNase-seq method dramatically improved the sequence-based prediction of TFBSs. Furthermore, they find that adding the histone modifications H3K4me3 or H3K27ac to their DNase-seq model improved the accuracy slightly. The comparison showed that Centipede outperformed their method using the gold standard proposed in Pique-Regi et al.24 However, in this case, they found that using the simple TC approach would outperform both Centipede and their approach. They associate such results with the biases generated by such MPBScentric gold standard. Piper et al. (Wellington) Piper et al.43 devised a segmentation approach based on a Binomial test. For a given candidate footprint, it tests the hypothesis that there are more reads in the flanking regions than within the footprint. Following an observation that DNase-seq cuts of the double-hit protocol are strand-specific, Wellington only considers reads mapped to the upstream flanking region of the footprints. They evaluate their method and competing methods in a gold standard created with 214 human TF ChIPseq data sets. First, they showed that using such observed strand imbalance of reads increases the computational footprinting predictive power. Furthermore, their strategy outperformed competing methods Hesselberth et al.,34 Neph et al.33 and Centipede.24 Finally, a great contribution on this paper is the creation of a DNase-seq data processing package in the programming language Python termed pyDNase. Such package allows a user-friendly application of their methodology and also further DNase-seq data processing tools. Sherwood et al. (PIQ) Sherwood et al.26 developed a computational footprinting framework termed protein interaction quantification (PIQ). PIQ is a site-centric method, which uses Gaussian process to model and smooth the footprint profiles around candidate MPBSs (100 bp up/downstream).26 Active footprints are estimated with an expectation propagation algorithm. Finally, PIQ indicates the set of motifs which footprint signals are distinguishable from noise to reduce the set of candidate TFs. They compared their method with competing methods in a very large benchmarking data set containing 303 TFs binding on K562 human cell line. Through the same evaluation procedure used in the aforementioned works,24, 25, 43 they measured a mean AUC of 0.93 for PIQ against 0.87 for Centipede24 and 0.65 for Neph et al.33 approach. Nevertheless, this study contains many side analyses which provided insights into computational footprinting. They analyzed the differentiation of mouse embryonic stem cells into prepancreatic and intestinal endoderm cells and were able to identify and experimentally validate eight pioneer TF families that perform changes in the chromatin dynamics. One of the most interesting findings is that these pioneer TFs change the chromatin in a directional manner. Besides the identification of pioneer TFs, they also detected “settler” TFs, which binds the DNA after the chromatin structure changes performed by the pioneer TFs. Yardımcı et al. (FLR) The issues on computational footprinting presented by He et al.32 were addressed in Yardımcı et al.31 In this study, they proposed a site-centric method based on a mixture of multinomial models to detect classify MPBSs as active/inactive in an unsupervised manner. The method uses an expectation  
bias frequencies or estimated de novo. After successful estimation, MPBSs are scored with the log odds ratio for the footprint versus background model. The model takes DNase-seq cuts within a small window around the candidate profiles (25 bp up/downstream) as input. DNase-seq cleavage bias is estimated for 6-mers based on the DNA sequences extracted within the same regions in which the cuts were retrieved. They showed that their method significantly outperformed the simple TC approach. Furthermore, they also criticise the TF ChIP-seq evaluation method on the basis that it is not able to identify indirect binding events. For that reason, they performed a simple analysis based on gene expression and were able to observe that the footprints retrieved by their approach were significantly enriched on cell types where the tested TFs were being expressed. Sung et al. (DNase2TF) Sung et al.29 also performed a number of analysis that contributed to the discussion initiated in He et al.32 First, they developed a new segmentation computational footprinting approach with very simple premisses, which was called DNase2TF. DNase2TF is based on a binomial z-score, which evaluates the depletion of DNase-seq reads around the candidate footprints. At a second step, DNase2TF interactively merges close candidate footprints whenever they improve depletion scores. DNase2TF corrects for DNase I cleavage bias using cleavage statistics for 2 or 4-mers. They reported that their method outperformed Hesselberth et al.,34 Centipede24 and Wellington.43 Their evaluation data set used only ChIP-seq information without MPBSs, leading to ROC-like curves in which the curves could stop in the middle of the sensitivity versus specificity spectrum given the lack of negative samples. Although their evaluation is hard to compare to previous approaches, they also raised the issue seen by He et al.32 that some TF DNase-seq signature ressembles their cleavage bias. Furthermore, they showed that one of the main problems with DNase I footprinting was related to the fact that some TFs have a very low residence time on DNA. Since they bind to the DNA in a short time period, the DNase-seq protocol is not able to produce a clear peak-dip-peak pattern. Kähärä et al. (BinDNase) Kähärä et al.27 developed a supervised site-centric method based on logistic regression to predict active/inactive MBBSs. The algorithm starts with base pair resolution DNase-seq signal around the MPBSs (100 bps up/downstream) and selects discriminatory features using a backward greedy approach. As a supervised approach, the method requires positive and negative examples, which can be obtained from TF ChIP-seq data. They showed that their approach does not present any significant gain in performance by modelling DNase-seq cleavage bias. Furthermore, they present a discussion on the standardization of DNase-seq data preprocessing, showing that data on major repositories such as ENCODE are not always analyzed in a standard manner. They state that their supervised approach outperforms unsupervised site-centric approaches such as Centipede24 and PIQ.26 However, they acknowledge the fact that their approach can only be executed when TF binding evidence (such as TF ChIP-seq) is available a priori for model training.  
In this chapter we introduced the main concepts within the molecular biology field of gene regulation. Then, we defined the problem we are going to address in this thesis, which is to identify active transcription factor binding sites, i.e. DNA regions being bound by regulatory proteins at a particular cell state or condition. We have discussed that sequence-based computational approaches  which takes advantage of the protein-DNA sequence binding affinity are not able to identify active sites, since the chromatin dynamics also needs to be considered. Nevertheless, we show that novel NGS-based assays such as DNase-seq and ChIP-seq are capable of capturing such cell-specific chromatin dynamics. However, the magnitude and complexity of the data generated by these biological experimental assays calls for a robust computational framework. Finally, we discussed a particular type of computational framework for NGS-based data – the computational footprinting methods – which addresses the TFBS identification problem by processing such NGS-based data and searching for patterns that are indicative of active TFBSs. We performed a comprehensive literature review on the main computational footprinting methods and dicussed the current challenges on this field. In this thesis we plan to investigate computational footprinting methods in detail. Among our main goals are: • The development of a computational footprinting methods which takes advantage of the full (nucleotide resolution) grammar of active TFBSs given by the DNase-seq and histone modification ChIP-seq data. Since we plan to use a system-exploratory approach, we will develop the method on the basis of the segmentation computational footprinting type. • The investigation of the proper NGS-based genomic signal pre-processing and normalization. • The investigation and correction of the DNase-seq signal for DNase I sequence cleavage bias and other experimental artifacts since these issues were correlated with a decrease in accuracy for other computational footprinting methods. • The development of a proper method for evaluating computational footprinting methods. Such method should consider state-of-the-art machine learning features in order to minimize interpretation bias. Furthermore, an attempt to create a benchmark data set will be made, in order to standardize method comparison within this field. • The investigation of multiple parameterization scenarios for: NGS-based computational signal processing, computational footprinting methods and evaluation methodologies. • The comparison of multiple computational footprinting methods. We plan to investigate particularities associated to each method such as the cases in which each one have good performance. • The investigation of the correlation between the accuracy of computational footprinting methods and multiple biological genomic features. • The application of our computational footprinting framework in real case scenarios. As a matter of fact, there has been a discussion in the literature regarding the applicability of computational footprinting methods versus more simple approaches.30, 32 According to He et al.32 and Cuellar-Partida et al.,25 the simple statistic to rank MPBSs by counting the number of DNase-seq reads around these putative binding regions – tag count (TC) – can be applied instead of more complex methods. Such discussion led to some disagreement on the literature on whether computational footprint methods were really useful. In this thesis we will perform a comprehensive investigation of these claims.  
